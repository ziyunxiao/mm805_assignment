{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. (40 points) Feature extraction and matching\n",
    "\n",
    "   (use the images from https://sourceforge.net/projects/adobedatasets.adobe/files/adobe_panoramas.tgz/download)  \n",
    "    a. (10 points) Select and implement one of the point feature detectors we have explained or use other methods you learned about. (Make sure to implement the feature detector yourself). Explain your selected detector and show the results.   \n",
    "    b. (20 points) Implement a simple feature matching by using two feature descriptors of your choice (you can use the available feature descriptors in OpenCV or Matlab). Compare the two feature descriptors and the matching results on a few different images.  \n",
    "    c. (10 points) Instead of finding feature points independently in multiple images and then matching them, find features in the first image of a video or image sequence and then re-locate the corresponding points in the next frames using search and gradient descent (Shi and Tomasi 1994). When the number of tracked points drops below a threshold or new regions in the image become visible, find additional points to track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a.  Harris Corner Point detection\n",
    "\n",
    "The feature I implemented is Harris corner point. The defail info can be found at [wikipedia](https://en.wikipedia.org/wiki/Corner_detection#The_Harris_&_Stephens_/_Shi%E2%80%93Tomasi_corner_detection_algorithms)\n",
    "\n",
    "The key function is `get_harris_points`, the detail code can be found in assignment_code.py ane extra function `display_corner_points` is written for display finding point on image as red point.\n",
    "\n",
    "Python code\n",
    "```python\n",
    "def get_harris_points(I, k=0.05):\n",
    "\n",
    "    # check image and convert to gray and normalize\n",
    "    if len(I.shape) == 3 and I.shape[2] == 3:\n",
    "        I = cv2.cvtColor(I, cv2.COLOR_BGR2GRAY)\n",
    "    if I.max() > 1.0:\n",
    "        I = I / 255.0\n",
    "\n",
    "    # Step 1 calcualte Axx, Axy and Ayy\n",
    "    \n",
    "    # Step 1.1 calculate Ix, Iy\n",
    "    # apply soble filter for quick calculation\n",
    "    filter_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]) / \\\n",
    "        8.0  # sobel filter for x derivative\n",
    "    filter_y = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]) / \\\n",
    "        8.0  # sobel filter for y derivative\n",
    "\n",
    "    Ix = imfilter(I, filter_x)\n",
    "    Iy = imfilter(I, filter_y)\n",
    "    Ixx = Ix * Ix\n",
    "    Iyy = Iy * Iy\n",
    "    Ixy = Ix * Iy\n",
    "\n",
    "    window = np.ones((3, 3))\n",
    "\n",
    "    Axx = imfilter(Ixx, window)\n",
    "    Axy = imfilter(Ixy, window)\n",
    "    Ayy = imfilter(Iyy, window)\n",
    "\n",
    "    # Step 2 calculate response\n",
    "    # determinant\n",
    "    detA = Axx * Ayy - Axy ** 2\n",
    "    # trace\n",
    "    traceA = Axx + Ayy\n",
    "    # response\n",
    "    response = detA - k * traceA ** 2\n",
    "\n",
    "    # step 3. Get points location(x,y)\n",
    "    points = corner_peaks(response,min_distance=4)\n",
    "    # points = get_coordinate(response,I.shape)\n",
    "\n",
    "    return points\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing result\n",
    "import assignment_code as ac\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import cv2\n",
    "from skimage.feature import corner_peaks, corner_subpix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_name = \"data/carmel/carmel-00.png\"\n",
    "# image = cv2.imread(image_name,cv2.IMREAD_GRAYSCALE)\n",
    "# plt.imshow(image, cmap='gray', vmin=0, vmax=255)\n",
    "# plt.title(\"original\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "I = cv2.imread(image_name)\n",
    "grey = cv2.cvtColor(I, cv2.COLOR_BGR2GRAY)\n",
    "grey = grey / 255.0\n",
    "points = ac.get_harris_points(grey)\n",
    "output_name = f\"./output/a_harris_output.jpg\"\n",
    "ac.display_corner_points(I,points,output_name)\n",
    "image2 = cv2.imread(output_name,cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# plt.imshow(image2, cmap='gray', vmin=0, vmax=255)\n",
    "# plt.title(\"Image with Harris Points\")\n",
    "# plt.show()\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 7), dpi=80, sharex=True, sharey=True)\n",
    "# ax[1].imshow(image, cmap='gray')\n",
    "# ax[0].imshow(image2, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disply Result\n",
    "\n",
    "The harris point is marked to red color\n",
    "\n",
    "Original Image  \n",
    "![org_image](./data/carmel/carmel-00.png)\n",
    "\n",
    "Image with Harris Points  \n",
    "![harris_image](./output/a_harris_output.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Feature Matching\n",
    "Implement a simple feature matching by using two feature descriptors of your choice (you can use the available feature descriptors in OpenCV or Matlab). Compare the two feature descriptors and the matching results on a few different images. \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "155be16b22ae18149487ad62c7bb9370a03e311791dd7fae10d825600bd65b14"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
